## RoboLib Overview (https://robolib.org/)
RoboLib is an abstractive summarization system I built from March 2020 to August 2021. My initial plan was to use off-the-shelf summarization tools to produce factually correct, and abstractive, summaries of non-fiction books. After realizing that state-of-the-art (SOTA) summarization tools produced summaries that were not abstractive enough and/or not factually correct enough, I attempted to build the tools myself.

After countless experiments, my final summarization pipeline looked like this:

1. Data cleaning. I fine-tuned a pre-trained language model (PLM) to automatically fix PDF-to-text conversion errors, and remove headers, footers, page numbers, bibliographies, in-text references, etc.
2. Book chunking. I split up the book into chunks of 4 to 9 sentences using a PLM to guide the splitting decisions. 
3. Extractive summarization. I selected 25 chunks using SOTA extractive summarization algorithms. 
4. Abstractive summarization. I summarized each of the 25 chunks 15 times using a massive PLM that I fine-tuned.
5. Summary reranking. With 15 candidate summaries per chunk, I used 2 PLMs to select the best candidate, based on each candidate's factuality and “human-like” scores.
## Data Cleaning
The books I wanted to summarize were available in PDF, so I needed to convert them into text files. There are tools to convert PDF files but they introduce an enormous amount of errors into the text file. At first I tried creating a list of rules to clean each converted text file, and that helped but it wasn’t good enough for my liking. My hand-written rules could fix many encoding errors and easy patterns, but struggled to remove page headers and footers.

When a PDF is converted to a text file, page headers are inserted directly before the text on that page, which usually means a header will be inserted right in the middle of a sentence (the sentence that starts on the previous page and ends on the current page). This means that random sentences will have a chapter title right in the middle of it. What made this problem much worse was that the extractive summarization algorithms always “liked” these sentences, because chapter and book titles are usually central to the book’s themes. As a result the top ranked chunks always contained sentences which had book chapters or titles erroneously inserted into the middle of them.

My next approach was to fine-tune a PLM to clean my messy text files automatically. I took a PLM that I pre-trained myself, then generated a synthetic dataset that would be used to fine-tune my PLM. I created this dataset by starting with clean english Wikipedia text, then randomly inserting common PDF-to-text conversion errors into it. My script would then label the inserted errors with a 1, and all original wikipedia text with a 0. Next, I fine-tuned my PLM to select the errors, which were tokens labeled 1. The results were ok, but this fine-tuned model still struggled with more challenging conversion errors. So I spent a week manually labeling PDF-to-text conversion errors to create an even better dataset. This did the trick. After further fine-tuning on my manually labeled data, my fine-tuned PLM could successfully clean text that originated from PDF files.
## Book Chunking
Extractive summarization methods are given sections or chunks of text which are then ranked, usually based on their centrality. Since it was difficult to retrieve paragraph boundaries from the PDF-to-text conversion, I needed to create pseudo-paragraphs that would act as inputs to the extractive summarization step.

I first split the entire book up into sentences using an off-the-shelf sentence tokenizer. My script would then create book chunks starting from the beginning of the book. For example, it would take sentences 4 through 9, fetch sentence embeddings using another PLM, then calculate which sentence differed most from its preceding sentence. This sentence became the last sentence of that chunk, and the process would start again from the next sentence. Now, I had pseudo-paragraphs, or chunks, that were between 4 and 9 sentences long. 
## Extractive Summarization
English non-fiction books typically contain between 50k and 200k words, but the maximum context length of language models is typically 1024 tokens, maybe 2048. This means I can’t condition my summary on the entire book. Instead, I choose to extract the main points of the book, and summarize them independently. By “main points”, I mean the most central, or key, book-chunks. 

There are dozens of extractive summarization algorithms. I started with TextRank, which first creates chunk embeddings, again using a PLM, then performs the PageRank algorithm to rank each chunk. The issue with TextRank is that many of the highest ranking chunks of text are similar, which means that our summaries would be repetitive. Instead of TextRank, I chose newer extractive methods that aimed to maximize centrality but minimize redundant content. After this extractive summarization step I was left with 25 chunks of text, per book, that reasonably summarized the entire book’s contents. 
## Abstractive Summarization
Since my goal was to freely display these book summaries online, I couldn’t just post these 25 extracted pseudo-paragraphs. That would be a copyright violation. It would also be a bit long, and contain plenty of unnecessary information. The purpose of abstractively summarizing each chunk was to both shorten the overall length of the summary via compression, and to avoid copyright issues by expressing the book’s main points in a different way. This was the most challenging step in the pipeline. I tried fine-tuning every SOTA abstractive summarization model on summary datasets but these models weren’t good enough. They would constantly make stuff up, aka hallucinate. 

The first problem I realized was that the datasets I was using to fine-tune these SOTA models (taken from HuggingFace) were horrible. The target summaries, or “ground truths”, contained plenty of hallucinations - other folks in NLP were starting to notice this as well (papers on hallucination in machine summarization started appearing on arxiv). So I collected around 100k abstractive summaries, from half-a-dozen datasets, and ranked them using my fact-checker (see below); I also checked for word overlap between source text and summary to remove highly extractive summaries. I took the most factually correct summaries and was left with around 2k summaries, and I had my own dataset of another 1k (see below). This left me with 3k gold summaries. These were summaries that did not copy too much from their source texts, and were reasonably factually correct.

Even with these 3k gold summaries, SOTA summarization systems like BART, Pegasus, etc, didn’t produce summaries that I would personally find any value in. They would too frequently hallucinate, or just make no sense. When a 6 billion-parameter language model, GPT-J-6B, was released I decided to fine-tune this model using my 3k gold summaries on Google’s TPUs. The results were much better now, but still could use improvement. 

I used this massive, fine-tuned, PLM to produce 15 candidate summaries per chunk. The candidates were created by sampling from the PLM’s logits. 
## Summary Reranking
In order to choose the best summary out of 15 candidates I used a fact-checker and a human-machine discriminator. The human-machine discriminator predicted if each summary was written by a human, or was machine-generated text; this idea was taken directly from the gpt-2 detector. Since I wanted summaries that sounded human, and didn’t make common machine mistakes, summaries with high human scores were ranked higher than low human scores. 

Fact-checking was a much more challenging task. I wanted my fact-checker to not just predict if the summary hallucinated, but also where it hallucinated; I wanted to build a word-level fact-checker. An earlier version of this pipeline involved using a word-level fact-checker and a masked-language-model to spot and correct errors, but this was before GPT-J-6B was available, and I scrapped that approach because it made too many grammatical mistakes. But I still wanted to use a word-level fact-checker so I could highlight words in my summaries that my model was uncertain about; this way the reader would be alerted not to fully trust the highlighted parts of the summaries. I didn’t want to spread misinformation via a hallucinating automatic summarizer. 

But no word-level fact-checking dataset existed. So I spent around 3 weeks creating one by grabbing random paragraphs from books and summarizing them myself to create around 500 factually correct paragraph summaries. By rephrasing my summaries, or the source paragraphs, I doubled my total to 1k. And finally, I created factually incorrect summaries by injecting errors into those 1k summaries and labeling the errors. I then had a word-level fact-checking dataset of 2k samples.

Next, I fine-tuned another large PLM on my fact-checking dataset, and used this fact-checker to “label” around 400k NLI samples. NLI is the task of determining if a hypothesis entails, contradicts, or neither, some evidence. So it’s similar to fact-checking but without the word-level labels. My word-level fact-checker labeled every word in this 400k NLI dataset. Next, I re-trained this same large PLM first on this weakly-labeled dataset of 400k samples, then on my 2k samples. This resulted in better accuracy than only using my 2k samples. 

Finally, I used this fact-checker, along with the human-machine discriminator to rank all candidate summaries. The top summaries were selected, and pasted into a single text file which would be displayed online.
## Overtime
I will quickly go through other experiments that didn’t make it into the final pipeline.

I pre-trained a 1.1 billion-parameter encoder-decoder based on MARGE (https://arxiv.org/abs/2006.15020); this took ~2.5 months on an RTX 3090 GPU. This model wasn’t as good as I was hoping, and GPT-J-6B was released, so I just used it instead. 

The BERT model I pre-trained was initially used as a masked-language-model to correct words that the fact-checker flagged. I needed to pre-train my own MLM because I needed to use the same vocabulary as my fact-checker (DeBERTa-XL), and I used rotary position embeddings because they are better than absolute position embeddings. This was a ~300 million-parameter BERT-like model that was pre-trained on around 30 billion tokens of the cleaned C4 dataset. This model was actually used in my final pipeline; it was fine-tuned for both the human-machine discriminator, and the PDF-to-text cleaner. 

In the fall of 2020, I tried to build a SOTA fact-checker. I spent most of my time on the ANLI dataset, which is the most difficult NLI dataset. My best system was significantly better than SOTA (by around 3%). I did this by employing adversarial learning on a RoBERTa-large model to attack both the attention scores, and the input word-embeddings. However, DeBERTa was released around that time and outperformed my model, so I just used DeBERTa-XL in my final pipeline. I also tried extending kNN-LM to kNN-NLI, before the kNN-NLI paper was released, but this didn’t improve results over my adversarial training approach. 

I also tried to tack kNN-LM onto my pre-trained MARGE model, which improved results. But sampling was too slow, even after performing a dimensionality reduction from 1280 features down to 128.

If I were to continue this project I would start by ranking all of my summaries - using my fact-checker and human-machine discriminator - then fine-tune GPT-J-6B on the highest ranked summaries. This is essentially reinforcement learning. Then, I could generate more summaries, rank them again, further fine-tune GPT-J-6B, and repeat. I feel this would work well since my ranking system is very strong.


